<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width">
<meta name="theme-color" content="#222" media="(prefers-color-scheme: light)">
<meta name="theme-color" content="#222" media="(prefers-color-scheme: dark)"><meta name="generator" content="Hexo 7.1.1">

  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">



<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.5.1/css/all.min.css" integrity="sha256-wiz7ZSCn/btzhjKDQBms9Hx4sSeUYsDrTLg7roPstac=" crossorigin="anonymous">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/animate.css/3.1.1/animate.min.css" integrity="sha256-PR7ttpcvz8qrF57fur/yAx1qXMFJeJFiA6pSzWi0OIE=" crossorigin="anonymous">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/fancyapps-ui/5.0.33/fancybox/fancybox.css" integrity="sha256-gkQVf8UKZgQ0HyuxL/VnacadJ+D2Kox2TCEBuNQg5+w=" crossorigin="anonymous">

<script class="next-config" data-name="main" type="application/json">{"hostname":"zhuyinna.github.io","root":"/","images":"/images","scheme":"Gemini","darkmode":true,"version":"8.19.2","exturl":false,"sidebar":{"position":"left","display":"post","padding":18,"offset":12},"copycode":{"enable":true,"style":null},"fold":{"enable":false,"height":500},"bookmark":{"enable":false,"color":"#222","save":"auto"},"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"stickytabs":false,"motion":{"enable":true,"async":false,"transition":{"menu_item":"fadeInDown","post_block":"fadeIn","post_header":"fadeInDown","post_body":"fadeInDown","coll_header":"fadeInLeft","sidebar":"fadeInUp"}},"i18n":{"placeholder":"Searching...","empty":"We didn't find any results for the search: ${query}","hits_time":"${hits} results found in ${time} ms","hits":"${hits} results found"},"path":"/search.xml","localsearch":{"enable":true,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false}}</script><script src="/js/config.js"></script>

    <meta name="description" content="过&#x2F;欠拟合 &amp; 梯度消失&#x2F;爆炸 &amp; 权重初始化 &amp; 梯度下降法 &amp;学习率 &amp; 正则化 &amp; Batch Normalization…">
<meta property="og:type" content="article">
<meta property="og:title" content="深度学习CV算法八股文">
<meta property="og:url" content="http://zhuyinna.github.io/2024/03/25/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0CV%E7%AE%97%E6%B3%95%E5%85%AB%E8%82%A1%E6%96%87/index.html">
<meta property="og:site_name" content="Joys Blog">
<meta property="og:description" content="过&#x2F;欠拟合 &amp; 梯度消失&#x2F;爆炸 &amp; 权重初始化 &amp; 梯度下降法 &amp;学习率 &amp; 正则化 &amp; Batch Normalization…">
<meta property="og:locale" content="en_US">
<meta property="og:image" content="https://s2.loli.net/2024/03/25/pjtE47vfsadSxkH.png">
<meta property="og:image" content="https://s2.loli.net/2024/03/25/Aqjaysel3nFItM2.png">
<meta property="og:image" content="https://s2.loli.net/2024/03/25/GnCu5OblpEU1tie.png">
<meta property="og:image" content="https://s2.loli.net/2024/03/25/deKWv5f8Rs2TSAZ.png">
<meta property="og:image" content="https://s2.loli.net/2024/03/25/fUAJZiF3LvCEtHu.png">
<meta property="og:image" content="https://s2.loli.net/2024/03/25/OuhysixpFcNDdY8.png">
<meta property="og:image" content="https://s2.loli.net/2024/03/25/ypv2Nz8hk1KBEXe.png">
<meta property="article:published_time" content="2024-03-25T02:55:45.000Z">
<meta property="article:modified_time" content="2024-03-25T16:40:42.319Z">
<meta property="article:author" content="Joy Zhu">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://s2.loli.net/2024/03/25/pjtE47vfsadSxkH.png">


<link rel="canonical" href="http://zhuyinna.github.io/2024/03/25/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0CV%E7%AE%97%E6%B3%95%E5%85%AB%E8%82%A1%E6%96%87/">



<script class="next-config" data-name="page" type="application/json">{"sidebar":"","isHome":false,"isPost":true,"lang":"en","comments":true,"permalink":"http://zhuyinna.github.io/2024/03/25/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0CV%E7%AE%97%E6%B3%95%E5%85%AB%E8%82%A1%E6%96%87/","path":"2024/03/25/深度学习CV算法八股文/","title":"深度学习CV算法八股文"}</script>

<script class="next-config" data-name="calendar" type="application/json">""</script>
<title>深度学习CV算法八股文 | Joys Blog</title>
  








  <noscript>
    <link rel="stylesheet" href="/css/noscript.css">
  </noscript>
<style>mjx-container[jax="SVG"] {
  direction: ltr;
}

mjx-container[jax="SVG"] > svg {
  overflow: visible;
}

mjx-container[jax="SVG"][display="true"] {
  display: block;
  text-align: center;
  margin: 1em 0;
}

mjx-container[jax="SVG"][justify="left"] {
  text-align: left;
}

mjx-container[jax="SVG"][justify="right"] {
  text-align: right;
}

g[data-mml-node="merror"] > g {
  fill: red;
  stroke: red;
}

g[data-mml-node="merror"] > rect[data-background] {
  fill: yellow;
  stroke: none;
}

g[data-mml-node="mtable"] > line[data-line] {
  stroke-width: 70px;
  fill: none;
}

g[data-mml-node="mtable"] > rect[data-frame] {
  stroke-width: 70px;
  fill: none;
}

g[data-mml-node="mtable"] > .mjx-dashed {
  stroke-dasharray: 140;
}

g[data-mml-node="mtable"] > .mjx-dotted {
  stroke-linecap: round;
  stroke-dasharray: 0,140;
}

g[data-mml-node="mtable"] > svg {
  overflow: visible;
}

[jax="SVG"] mjx-tool {
  display: inline-block;
  position: relative;
  width: 0;
  height: 0;
}

[jax="SVG"] mjx-tool > mjx-tip {
  position: absolute;
  top: 0;
  left: 0;
}

mjx-tool > mjx-tip {
  display: inline-block;
  padding: .2em;
  border: 1px solid #888;
  font-size: 70%;
  background-color: #F8F8F8;
  color: black;
  box-shadow: 2px 2px 5px #AAAAAA;
}

g[data-mml-node="maction"][data-toggle] {
  cursor: pointer;
}

mjx-status {
  display: block;
  position: fixed;
  left: 1em;
  bottom: 1em;
  min-width: 25%;
  padding: .2em .4em;
  border: 1px solid #888;
  font-size: 90%;
  background-color: #F8F8F8;
  color: black;
}

foreignObject[data-mjx-xml] {
  font-family: initial;
  line-height: normal;
  overflow: visible;
}

.MathJax path {
  stroke-width: 3;
}

mjx-container[display="true"] {
  overflow: auto hidden;
}

mjx-container[display="true"] + br {
  display: none;
}
</style></head>

<body itemscope itemtype="http://schema.org/WebPage" class="use-motion">
  <div class="headband"></div>

  <main class="main">
    <div class="column">
      <header class="header" itemscope itemtype="http://schema.org/WPHeader"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="Toggle navigation bar" role="button">
        <span class="toggle-line"></span>
        <span class="toggle-line"></span>
        <span class="toggle-line"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <i class="logo-line"></i>
      <p class="site-title">Joys Blog</p>
      <i class="logo-line"></i>
    </a>
      <p class="site-subtitle" itemprop="description">subtitle</p>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger" aria-label="Search" role="button">
        <i class="fa fa-search fa-fw fa-lg"></i>
    </div>
  </div>
</div>



<nav class="site-nav">
  <ul class="main-menu menu"><li class="menu-item menu-item-home"><a href="/" rel="section"><i class="fa fa-home fa-fw"></i>Home</a></li><li class="menu-item menu-item-about"><a href="/about/" rel="section"><i class="fa fa-user fa-fw"></i>About</a></li><li class="menu-item menu-item-tags"><a href="/tags/" rel="section"><i class="fa fa-tags fa-fw"></i>Tags</a></li><li class="menu-item menu-item-categories"><a href="/categories/" rel="section"><i class="fa fa-th fa-fw"></i>Categories</a></li><li class="menu-item menu-item-archives"><a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>Archives</a></li>
      <li class="menu-item menu-item-search">
        <a role="button" class="popup-trigger"><i class="fa fa-search fa-fw"></i>Search
        </a>
      </li>
  </ul>
</nav>



  <div class="search-pop-overlay">
    <div class="popup search-popup"><div class="search-header">
  <span class="search-icon">
    <i class="fa fa-search"></i>
  </span>
  <div class="search-input-container">
    <input autocomplete="off" autocapitalize="off" maxlength="80"
           placeholder="Searching..." spellcheck="false"
           type="search" class="search-input">
  </div>
  <span class="popup-btn-close" role="button">
    <i class="fa fa-times-circle"></i>
  </span>
</div>
<div class="search-result-container no-result">
  <div class="search-result-icon">
    <i class="fa fa-spinner fa-pulse fa-5x"></i>
  </div>
</div>

    </div>
  </div>

</header>
        
  
  <aside class="sidebar">

    <div class="sidebar-inner sidebar-nav-active sidebar-toc-active">
      <ul class="sidebar-nav">
        <li class="sidebar-nav-toc">
          Table of Contents
        </li>
        <li class="sidebar-nav-overview">
          Overview
        </li>
      </ul>

      <div class="sidebar-panel-container">
        <!--noindex-->
        <div class="post-toc-wrap sidebar-panel">
            <div class="post-toc animated"><ol class="nav"><li class="nav-item nav-level-1"><a class="nav-link" href="#1-%E8%BF%87%E6%8B%9F%E5%90%88"><span class="nav-text">1. 过拟合</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#1-1-L1-L2%E6%AD%A3%E5%88%99%E5%8C%96"><span class="nav-text">1.1 L1&#x2F;L2正则化</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#1-2-Dropout"><span class="nav-text">1.2 Dropout</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#1-2-1-Dropout%E6%AD%A3%E5%88%99%E5%8C%96"><span class="nav-text">1.2.1 Dropout正则化</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#1-2-2-Vanilla-Dropout"><span class="nav-text">1.2.2 Vanilla Dropout</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#1-2-3-Inverted-Dropout%EF%BC%88%E5%8F%8D%E5%90%91%E9%9A%8F%E6%9C%BA%E5%A4%B1%E6%B4%BB%EF%BC%89"><span class="nav-text">1.2.3 Inverted Dropout（反向随机失活）</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#1-3-Early-Stopping"><span class="nav-text">1.3 Early Stopping</span></a></li></ol></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#2-%E6%AC%A0%E6%8B%9F%E5%90%88"><span class="nav-text">2. 欠拟合</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#3-%E6%A2%AF%E5%BA%A6%E6%B6%88%E5%A4%B1%E5%92%8C%E6%A2%AF%E5%BA%A6%E7%88%86%E7%82%B8"><span class="nav-text">3. 梯度消失和梯度爆炸</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#3-1-%E5%8E%9F%E5%9B%A0"><span class="nav-text">3.1 原因</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#3-2-%E6%BF%80%E6%B4%BB%E5%87%BD%E6%95%B0"><span class="nav-text">3.2 激活函数</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#3-3-%E8%A7%A3%E5%86%B3%E6%96%B9%E6%A1%88"><span class="nav-text">3.3 解决方案</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#3-3-1-%E9%A2%84%E8%AE%AD%E7%BB%83-%E5%BE%AE%E8%B0%83"><span class="nav-text">3.3.1 预训练+微调</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#3-3-2-%E6%A2%AF%E5%BA%A6%E5%89%AA%E5%88%87-%E6%AD%A3%E5%88%99"><span class="nav-text">3.3.2 梯度剪切+正则</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#3-3-3-relu-leakrelu-elu%E6%BF%80%E6%B4%BB%E5%87%BD%E6%95%B0"><span class="nav-text">3.3.3 relu&#x2F;leakrelu&#x2F;elu激活函数</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#3-3-4-Batchnorm"><span class="nav-text">3.3.4 Batchnorm</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#3-5-%E6%AE%8B%E5%B7%AE%E7%BB%93%E6%9E%84"><span class="nav-text">3.5 残差结构</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#3-6-LSTM"><span class="nav-text">3.6 LSTM</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#4-%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E6%9D%83%E9%87%8D%E5%88%9D%E5%A7%8B%E5%8C%96"><span class="nav-text">4. 神经网络权重初始化</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#4-1-Xavier"><span class="nav-text">4.1 Xavier</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#4-2-He-initialization"><span class="nav-text">4.2 He initialization</span></a></li><li class="nav-item nav-level-2"><a class="nav-link"><span class="nav-text"> </span></a></li></ol></li></ol></div>
        </div>
        <!--/noindex-->

        <div class="site-overview-wrap sidebar-panel">
          <div class="site-author animated" itemprop="author" itemscope itemtype="http://schema.org/Person">
    <img class="site-author-image" itemprop="image" alt="Joy Zhu"
      src="/images/%E5%A4%B4%E5%83%8F2.jpg">
  <p class="site-author-name" itemprop="name">Joy Zhu</p>
  <div class="site-description" itemprop="description">Record of life, work, study, and thoughts</div>
</div>
<div class="site-state-wrap animated">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
        <a href="/archives/">
          <span class="site-state-item-count">11</span>
          <span class="site-state-item-name">posts</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
          <a href="/categories/">
        <span class="site-state-item-count">13</span>
        <span class="site-state-item-name">categories</span></a>
      </div>
      <div class="site-state-item site-state-tags">
          <a href="/tags/">
        <span class="site-state-item-count">5</span>
        <span class="site-state-item-name">tags</span></a>
      </div>
  </nav>
</div>

        </div>
      </div>
    </div>

    
  </aside>


    </div>

    <div class="main-inner post posts-expand">


  


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="en">
    <link itemprop="mainEntityOfPage" href="http://zhuyinna.github.io/2024/03/25/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0CV%E7%AE%97%E6%B3%95%E5%85%AB%E8%82%A1%E6%96%87/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/%E5%A4%B4%E5%83%8F2.jpg">
      <meta itemprop="name" content="Joy Zhu">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Joys Blog">
      <meta itemprop="description" content="Record of life, work, study, and thoughts">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="深度学习CV算法八股文 | Joys Blog">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          深度学习CV算法八股文
        </h1>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">Posted on</span>

      <time title="Created: 2024-03-25 10:55:45" itemprop="dateCreated datePublished" datetime="2024-03-25T10:55:45+08:00">2024-03-25</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar-check"></i>
      </span>
      <span class="post-meta-item-text">Edited on</span>
      <time title="Modified: 2024-03-26 00:40:42" itemprop="dateModified" datetime="2024-03-26T00:40:42+08:00">2024-03-26</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">In</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/" itemprop="url" rel="index"><span itemprop="name">学习笔记</span></a>
        </span>
          , 
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/CV%E7%AE%97%E6%B3%95/" itemprop="url" rel="index"><span itemprop="name">CV算法</span></a>
        </span>
    </span>

  
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody"><p>过/欠拟合 &amp; 梯度消失/爆炸 &amp; 权重初始化 &amp; 梯度下降法 &amp;学习率 &amp; 正则化 &amp; Batch Normalization…<br><span id="more"></span></p>
<h1 id="1-过拟合"><a href="#1-过拟合" class="headerlink" title="1. 过拟合"></a>1. 过拟合</h1><p>可能原因</p>
<ul>
<li>训练数据大小过小，并且未包含足够的数据样本，无法准确代表所有可能的输入数据值。</li>
<li>训练数据包含大量不相关的信息，称为噪声数据。</li>
<li>模型在单个数据样本集上训练的时间过长。</li>
<li>模型复杂程度较高，因此其可以学习训练数据中的噪声。</li>
</ul>
<p>本质原因<br>神经网络就是一个函数，对其进行傅里叶变换求得频谱，频谱中低频分量就是变化平滑的部分，高频分量就是变化敏感的部分。模型对于微小扰动的反馈差异大实际就是一个过拟合的表现，也就是高频分量不能多</p>
<p>解决方法</p>
<ul>
<li>数据增强</li>
<li>L1/L2正则化</li>
<li>Dropout正则化</li>
<li>Early stopping</li>
<li>Batch normalization</li>
</ul>
<h2 id="1-1-L1-L2正则化"><a href="#1-1-L1-L2正则化" class="headerlink" title="1.1 L1/L2正则化"></a>1.1 L1/L2正则化</h2><p>L1正则化直接在损失函数上加上权重的绝对值,会使权重稀疏</p>
<script type="math/tex; mode=display">\begin{aligned}\mathrm{loss}&=\mathrm{J}\left(\mathrm{w},\mathrm{b}\right)+\frac\lambda{2\mathrm{m}}\sum|\mathrm{w}|\end{aligned}</script><p>L2正则化在损失函数上加上权重的平方和,会使权重更加平滑</p>
<script type="math/tex; mode=display">\mathrm{loss}=\mathrm{J}\left(\mathrm{w},\mathrm{b}\right)+\frac{\lambda}{2\mathrm{m}}\sum\lVert\mathrm{w}\rVert_{\mathrm{F}}^2</script><h2 id="1-2-Dropout"><a href="#1-2-Dropout" class="headerlink" title="1.2 Dropout"></a>1.2 Dropout</h2><h3 id="1-2-1-Dropout正则化"><a href="#1-2-1-Dropout正则化" class="headerlink" title="1.2.1 Dropout正则化"></a>1.2.1 Dropout正则化</h3><p>Dropout正则化是指在训练过程中，随机将神经网络中的一部分神经元的输出设置为0，从而减少神经元之间的依赖关系，防止过拟合。</p>
<p>训练</p>
<ul>
<li>训练时，每个神经元都有概率1-p被保留，概率p被丢弃. 参数多的层，p可以设置小一点，参数少的层，p可以设置大一点</li>
<li>删除神经网络节点和从该节点进出的连线</li>
</ul>
<p>推理</p>
<ul>
<li>推理时，所有神经元都保留，但是输出值乘以1-p(rescale)</li>
</ul>
<p>原因<br>dropout带有随机性, 如果infer也使用, 会导致每次推理结果不一致, 所以在推理时, 保留所有神经元, 但是为了保持期望值不变,保证训练和推理的分布一致, 进行rescale</p>
<p>本质<br>Dropout最终产⽣收缩权重的平方范数的效果，压缩权重效果类似L2正则化</p>
<h3 id="1-2-2-Vanilla-Dropout"><a href="#1-2-2-Vanilla-Dropout" class="headerlink" title="1.2.2 Vanilla Dropout"></a>1.2.2 Vanilla Dropout</h3><p>forward时候进行mask, backward时梯度和输入保持一致(也要mask), infer时进行1-p的rescale. (如1.2.1所述, 训练时候因为随机扔掉了一些节点，总期望变小，那么预测时候就全体缩小一点来保持一致)</p>
<p><strong>问题</strong>: 训练时候因为随机扔掉了一些节点，总期望变小，那么预测时候就全体缩小一点来保持一致。</p>
<h3 id="1-2-3-Inverted-Dropout（反向随机失活）"><a href="#1-2-3-Inverted-Dropout（反向随机失活）" class="headerlink" title="1.2.3 Inverted Dropout（反向随机失活）"></a>1.2.3 Inverted Dropout（反向随机失活）</h3><p>和Vanilla Dropout区别: 训练, 梯度, 预测</p>
<ul>
<li>产生⼀个[0,1)的随机矩阵，维度与权重矩阵相同</li>
<li>设置节点保留概率keep_prob 并与随机矩阵比较，小于为1，大于为0</li>
<li>将权重矩阵与0-1矩阵对应相乘得到新权重矩阵</li>
<li>对新权重矩阵除于keep_prob（保证输⼊均值和输出均值一致），保证权重矩阵均值不变，层输出不变</li>
</ul>
<p><strong>改进</strong>: 把所有修改放在训练阶段,保持预测阶段不变。 保持预测阶段不变。</p>
<h3 id="1-3-Early-Stopping"><a href="#1-3-Early-Stopping" class="headerlink" title="1.3 Early Stopping"></a>1.3 Early Stopping</h3><p>在训练中计算模型在验证集上的表现，当模型在验证集上的误差开始增大时，停止训练。这样就可以避免继续训练导致的过拟合问题. 训练时间和泛化误差的权衡。提早停⽌训练神经网络得到⼀个中等大小的W的F范数，与L2正则化类似<br><img src="https://s2.loli.net/2024/03/25/pjtE47vfsadSxkH.png" alt=""></p>
<h1 id="2-欠拟合"><a href="#2-欠拟合" class="headerlink" title="2. 欠拟合"></a>2. 欠拟合</h1><p>增加模型复杂度，增加训练数据量，减少正则化参数，减少dropout参数</p>
<h1 id="3-梯度消失和梯度爆炸"><a href="#3-梯度消失和梯度爆炸" class="headerlink" title="3. 梯度消失和梯度爆炸"></a>3. 梯度消失和梯度爆炸</h1><h2 id="3-1-原因"><a href="#3-1-原因" class="headerlink" title="3.1 原因"></a>3.1 原因</h2><p>深层网络, 或者不合适的loss函数</p>
<script type="math/tex; mode=display">\Delta w_1=\frac{\partial Loss}{\partial w_2}=\frac{\partial Loss}{\partial f_4}\frac{\partial f_4}{\partial f_3}\frac{\partial f_3}{\partial f_2}\frac{\partial f_2}{\partial w_2},</script><p><mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -1.11ex;" xmlns="http://www.w3.org/2000/svg" width="3.383ex" height="3.351ex" role="img" focusable="false" viewBox="0 -990.5 1495.4 1481.1"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="mfrac"><g data-mml-node="mrow" transform="translate(220,485) scale(0.707)"><g data-mml-node="mi"><path data-c="1D715" d="M202 508Q179 508 169 520T158 547Q158 557 164 577T185 624T230 675T301 710L333 715H345Q378 715 384 714Q447 703 489 661T549 568T566 457Q566 362 519 240T402 53Q321 -22 223 -22Q123 -22 73 56Q42 102 42 148V159Q42 276 129 370T322 465Q383 465 414 434T455 367L458 378Q478 461 478 515Q478 603 437 639T344 676Q266 676 223 612Q264 606 264 572Q264 547 246 528T202 508ZM430 306Q430 372 401 400T333 428Q270 428 222 382Q197 354 183 323T150 221Q132 149 132 116Q132 21 232 21Q244 21 250 22Q327 35 374 112Q389 137 409 196T430 306Z"></path></g><g data-mml-node="msub" transform="translate(566,0)"><g data-mml-node="mi"><path data-c="1D453" d="M118 -162Q120 -162 124 -164T135 -167T147 -168Q160 -168 171 -155T187 -126Q197 -99 221 27T267 267T289 382V385H242Q195 385 192 387Q188 390 188 397L195 425Q197 430 203 430T250 431Q298 431 298 432Q298 434 307 482T319 540Q356 705 465 705Q502 703 526 683T550 630Q550 594 529 578T487 561Q443 561 443 603Q443 622 454 636T478 657L487 662Q471 668 457 668Q445 668 434 658T419 630Q412 601 403 552T387 469T380 433Q380 431 435 431Q480 431 487 430T498 424Q499 420 496 407T491 391Q489 386 482 386T428 385H372L349 263Q301 15 282 -47Q255 -132 212 -173Q175 -205 139 -205Q107 -205 81 -186T55 -132Q55 -95 76 -78T118 -61Q162 -61 162 -103Q162 -122 151 -136T127 -157L118 -162Z"></path></g><g data-mml-node="mn" transform="translate(523,-150) scale(0.707)"><path data-c="34" d="M462 0Q444 3 333 3Q217 3 199 0H190V46H221Q241 46 248 46T265 48T279 53T286 61Q287 63 287 115V165H28V211L179 442Q332 674 334 675Q336 677 355 677H373L379 671V211H471V165H379V114Q379 73 379 66T385 54Q393 47 442 46H471V0H462ZM293 211V545L74 212L183 211H293Z"></path></g></g></g><g data-mml-node="mrow" transform="translate(220,-345.6) scale(0.707)"><g data-mml-node="mi"><path data-c="1D715" d="M202 508Q179 508 169 520T158 547Q158 557 164 577T185 624T230 675T301 710L333 715H345Q378 715 384 714Q447 703 489 661T549 568T566 457Q566 362 519 240T402 53Q321 -22 223 -22Q123 -22 73 56Q42 102 42 148V159Q42 276 129 370T322 465Q383 465 414 434T455 367L458 378Q478 461 478 515Q478 603 437 639T344 676Q266 676 223 612Q264 606 264 572Q264 547 246 528T202 508ZM430 306Q430 372 401 400T333 428Q270 428 222 382Q197 354 183 323T150 221Q132 149 132 116Q132 21 232 21Q244 21 250 22Q327 35 374 112Q389 137 409 196T430 306Z"></path></g><g data-mml-node="msub" transform="translate(566,0)"><g data-mml-node="mi"><path data-c="1D453" d="M118 -162Q120 -162 124 -164T135 -167T147 -168Q160 -168 171 -155T187 -126Q197 -99 221 27T267 267T289 382V385H242Q195 385 192 387Q188 390 188 397L195 425Q197 430 203 430T250 431Q298 431 298 432Q298 434 307 482T319 540Q356 705 465 705Q502 703 526 683T550 630Q550 594 529 578T487 561Q443 561 443 603Q443 622 454 636T478 657L487 662Q471 668 457 668Q445 668 434 658T419 630Q412 601 403 552T387 469T380 433Q380 431 435 431Q480 431 487 430T498 424Q499 420 496 407T491 391Q489 386 482 386T428 385H372L349 263Q301 15 282 -47Q255 -132 212 -173Q175 -205 139 -205Q107 -205 81 -186T55 -132Q55 -95 76 -78T118 -61Q162 -61 162 -103Q162 -122 151 -136T127 -157L118 -162Z"></path></g><g data-mml-node="mn" transform="translate(523,-150) scale(0.707)"><path data-c="33" d="M127 463Q100 463 85 480T69 524Q69 579 117 622T233 665Q268 665 277 664Q351 652 390 611T430 522Q430 470 396 421T302 350L299 348Q299 347 308 345T337 336T375 315Q457 262 457 175Q457 96 395 37T238 -22Q158 -22 100 21T42 130Q42 158 60 175T105 193Q133 193 151 175T169 130Q169 119 166 110T159 94T148 82T136 74T126 70T118 67L114 66Q165 21 238 21Q293 21 321 74Q338 107 338 175V195Q338 290 274 322Q259 328 213 329L171 330L168 332Q166 335 166 348Q166 366 174 366Q202 366 232 371Q266 376 294 413T322 525V533Q322 590 287 612Q265 626 240 626Q208 626 181 615T143 592T132 580H135Q138 579 143 578T153 573T165 566T175 555T183 540T186 520Q186 498 172 481T127 463Z"></path></g></g></g><rect width="1255.4" height="60" x="120" y="220"></rect></g></g></g></svg></mjx-container>是对激活函数求导<br>大于1: 随着层数增加,梯度爆炸<br>小于1: 随着层数增加, 梯度消失</p>
<blockquote>
<p>从深层网络角度来讲，不同的层学习的速度差异很大，表现为网络中靠近输出的层学习的情况很好，靠近输入的层学习的很慢，有时甚至训练了很久，前几层的权值和刚开始随机初始化的值差不多</p>
<p>TODO: 查阅Hinton提出的capsule, 抛弃反向传播</p>
</blockquote>
<h2 id="3-2-激活函数"><a href="#3-2-激活函数" class="headerlink" title="3.2 激活函数"></a>3.2 激活函数</h2><ol>
<li><p>sigmoid函数</p>
<script type="math/tex; mode=display">sigmoid(x)=\frac1{1+e^{-x}}</script><p>其梯度是不可能超过0.25的，这样经过链式求导之后，很容易发生梯度消失<br><img src="https://s2.loli.net/2024/03/25/Aqjaysel3nFItM2.png" width="50%"><img src="https://s2.loli.net/2024/03/25/GnCu5OblpEU1tie.png" width="50%"></p>
</li>
<li><p>tanh</p>
<script type="math/tex; mode=display">tanh(x)=\frac{e^x-e^{-x}}{e^x+e^{-x}}</script><p>好一些, 但是倒数仍然小于1<br><img src="https://s2.loli.net/2024/03/25/deKWv5f8Rs2TSAZ.png" width="100%"></p>
</li>
</ol>
<h2 id="3-3-解决方案"><a href="#3-3-解决方案" class="headerlink" title="3.3 解决方案"></a>3.3 解决方案</h2><div class="note info"><p><strong>解决梯度消失的方法</strong></p>
<ul>
<li>Relu及其变体</li>
<li>LSTM/GRU</li>
<li>残差结构</li>
<li>BatchNorm</li>
<li>Xavier初始化(修正w的方差，避免w过小)<br><strong>解决梯度爆炸的方法</strong></li>
<li>梯度裁剪</li>
<li>正则化(将w加入Loss里，如果Loss小则w也要小，而梯度爆炸是w过大[绝对值]造成的)</li>
<li>Xavier初始化(修正w的方差，避免w过大)</li>
<li>BatchNorm</li>
</ul>
</div>
<h3 id="3-3-1-预训练-微调"><a href="#3-3-1-预训练-微调" class="headerlink" title="3.3.1 预训练+微调"></a>3.3.1 预训练+微调</h3><p>逐层预训练,再对整个网络微调. 核心思想:先寻找局部最优, 整合后寻找全局最优</p>
<h3 id="3-3-2-梯度剪切-正则"><a href="#3-3-2-梯度剪切-正则" class="headerlink" title="3.3.2 梯度剪切+正则"></a>3.3.2 梯度剪切+正则</h3><p>剪切: 设置梯度阈值, 如果超出则强制限制, 防止梯度爆炸.<br>权重正则化: L1正则, L2正则 </p>
<script type="math/tex; mode=display">Loss=(y-W^Tx)^2+\alpha||W||^2</script><h3 id="3-3-3-relu-leakrelu-elu激活函数"><a href="#3-3-3-relu-leakrelu-elu激活函数" class="headerlink" title="3.3.3 relu/leakrelu/elu激活函数"></a>3.3.3 relu/leakrelu/elu激活函数</h3><ol>
<li>Relu</li>
</ol>
<ul>
<li>解决梯度消失/爆炸; 计算方便速度快; 加速训练</li>
<li>由于负数部分恒为0，会导致一些神经元无法激活（可通过设置小学习率部分解决）; 输出不是以0为中心的</li>
</ul>
<script type="math/tex; mode=display">\left.\text{Relu(x)=}\max(x,0)=\left\lbrace\begin{aligned}0,x<0\\\\x,x>0\end{aligned}\right.\right\rbrace</script><p><img src="https://s2.loli.net/2024/03/25/fUAJZiF3LvCEtHu.png" width="100%"></p>
<ol>
<li>Leakrelu</li>
</ol>
<ul>
<li>改进relu的0区间问题</li>
</ul>
<script type="math/tex; mode=display">\left.leakrelu=f(x)=\left\{\begin{array}{ll}x,&x>0\\\\x*k,&x\leq0\end{array}\right.\right.</script><p><img src="https://s2.loli.net/2024/03/25/OuhysixpFcNDdY8.png" width="100%"></p>
<h3 id="3-3-4-Batchnorm"><a href="#3-3-4-Batchnorm" class="headerlink" title="3.3.4 Batchnorm"></a>3.3.4 Batchnorm</h3><p>反向传播式子中有w的存在，所以w的大小影响了梯度的消失和爆炸，batchnorm就是通过对每一层的输出做scale和shift的方法，通过一定的规范化手段，把每层神经网络任意神经元这个输入值的分布强行拉回到接近均值为0方差为1的标准正太分布，即严重偏离的分布强制拉回比较标准的分布，这样使得激活输入值落在非线性函数对输入比较敏感的区域，这样输入的小变化就会导致损失函数较大的变化，使得让梯度变大，避免梯度消失问题产生，而且梯度变大意味着学习收敛速度快，能大大加快训练速度。</p>
<h2 id="3-5-残差结构"><a href="#3-5-残差结构" class="headerlink" title="3.5 残差结构"></a>3.5 残差结构</h2><p>式子的第一个因子<mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -1.022ex;" xmlns="http://www.w3.org/2000/svg" width="4.654ex" height="3.057ex" role="img" focusable="false" viewBox="0 -899.6 2057.2 1351.2"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="mfrac"><g data-mml-node="mrow" transform="translate(220,394) scale(0.707)"><g data-mml-node="mi"><path data-c="1D715" d="M202 508Q179 508 169 520T158 547Q158 557 164 577T185 624T230 675T301 710L333 715H345Q378 715 384 714Q447 703 489 661T549 568T566 457Q566 362 519 240T402 53Q321 -22 223 -22Q123 -22 73 56Q42 102 42 148V159Q42 276 129 370T322 465Q383 465 414 434T455 367L458 378Q478 461 478 515Q478 603 437 639T344 676Q266 676 223 612Q264 606 264 572Q264 547 246 528T202 508ZM430 306Q430 372 401 400T333 428Q270 428 222 382Q197 354 183 323T150 221Q132 149 132 116Q132 21 232 21Q244 21 250 22Q327 35 374 112Q389 137 409 196T430 306Z"></path></g><g data-mml-node="mi" transform="translate(566,0)"><path data-c="1D459" d="M117 59Q117 26 142 26Q179 26 205 131Q211 151 215 152Q217 153 225 153H229Q238 153 241 153T246 151T248 144Q247 138 245 128T234 90T214 43T183 6T137 -11Q101 -11 70 11T38 85Q38 97 39 102L104 360Q167 615 167 623Q167 626 166 628T162 632T157 634T149 635T141 636T132 637T122 637Q112 637 109 637T101 638T95 641T94 647Q94 649 96 661Q101 680 107 682T179 688Q194 689 213 690T243 693T254 694Q266 694 266 686Q266 675 193 386T118 83Q118 81 118 75T117 65V59Z"></path></g><g data-mml-node="mi" transform="translate(864,0)"><path data-c="1D45C" d="M201 -11Q126 -11 80 38T34 156Q34 221 64 279T146 380Q222 441 301 441Q333 441 341 440Q354 437 367 433T402 417T438 387T464 338T476 268Q476 161 390 75T201 -11ZM121 120Q121 70 147 48T206 26Q250 26 289 58T351 142Q360 163 374 216T388 308Q388 352 370 375Q346 405 306 405Q243 405 195 347Q158 303 140 230T121 120Z"></path></g><g data-mml-node="mi" transform="translate(1349,0)"><path data-c="1D460" d="M131 289Q131 321 147 354T203 415T300 442Q362 442 390 415T419 355Q419 323 402 308T364 292Q351 292 340 300T328 326Q328 342 337 354T354 372T367 378Q368 378 368 379Q368 382 361 388T336 399T297 405Q249 405 227 379T204 326Q204 301 223 291T278 274T330 259Q396 230 396 163Q396 135 385 107T352 51T289 7T195 -10Q118 -10 86 19T53 87Q53 126 74 143T118 160Q133 160 146 151T160 120Q160 94 142 76T111 58Q109 57 108 57T107 55Q108 52 115 47T146 34T201 27Q237 27 263 38T301 66T318 97T323 122Q323 150 302 164T254 181T195 196T148 231Q131 256 131 289Z"></path></g><g data-mml-node="mi" transform="translate(1818,0)"><path data-c="1D460" d="M131 289Q131 321 147 354T203 415T300 442Q362 442 390 415T419 355Q419 323 402 308T364 292Q351 292 340 300T328 326Q328 342 337 354T354 372T367 378Q368 378 368 379Q368 382 361 388T336 399T297 405Q249 405 227 379T204 326Q204 301 223 291T278 274T330 259Q396 230 396 163Q396 135 385 107T352 51T289 7T195 -10Q118 -10 86 19T53 87Q53 126 74 143T118 160Q133 160 146 151T160 120Q160 94 142 76T111 58Q109 57 108 57T107 55Q108 52 115 47T146 34T201 27Q237 27 263 38T301 66T318 97T323 122Q323 150 302 164T254 181T195 196T148 231Q131 256 131 289Z"></path></g></g><g data-mml-node="mrow" transform="translate(426.6,-345.6) scale(0.707)"><g data-mml-node="mi"><path data-c="1D715" d="M202 508Q179 508 169 520T158 547Q158 557 164 577T185 624T230 675T301 710L333 715H345Q378 715 384 714Q447 703 489 661T549 568T566 457Q566 362 519 240T402 53Q321 -22 223 -22Q123 -22 73 56Q42 102 42 148V159Q42 276 129 370T322 465Q383 465 414 434T455 367L458 378Q478 461 478 515Q478 603 437 639T344 676Q266 676 223 612Q264 606 264 572Q264 547 246 528T202 508ZM430 306Q430 372 401 400T333 428Q270 428 222 382Q197 354 183 323T150 221Q132 149 132 116Q132 21 232 21Q244 21 250 22Q327 35 374 112Q389 137 409 196T430 306Z"></path></g><g data-mml-node="msub" transform="translate(566,0)"><g data-mml-node="mi"><path data-c="1D465" d="M52 289Q59 331 106 386T222 442Q257 442 286 424T329 379Q371 442 430 442Q467 442 494 420T522 361Q522 332 508 314T481 292T458 288Q439 288 427 299T415 328Q415 374 465 391Q454 404 425 404Q412 404 406 402Q368 386 350 336Q290 115 290 78Q290 50 306 38T341 26Q378 26 414 59T463 140Q466 150 469 151T485 153H489Q504 153 504 145Q504 144 502 134Q486 77 440 33T333 -11Q263 -11 227 52Q186 -10 133 -10H127Q78 -10 57 16T35 71Q35 103 54 123T99 143Q142 143 142 101Q142 81 130 66T107 46T94 41L91 40Q91 39 97 36T113 29T132 26Q168 26 194 71Q203 87 217 139T245 247T261 313Q266 340 266 352Q266 380 251 392T217 404Q177 404 142 372T93 290Q91 281 88 280T72 278H58Q52 284 52 289Z"></path></g><g data-mml-node="TeXAtom" transform="translate(605,-150) scale(0.707)" data-mjx-texclass="ORD"><g data-mml-node="mi"><path data-c="1D43F" d="M228 637Q194 637 192 641Q191 643 191 649Q191 673 202 682Q204 683 217 683Q271 680 344 680Q485 680 506 683H518Q524 677 524 674T522 656Q517 641 513 637H475Q406 636 394 628Q387 624 380 600T313 336Q297 271 279 198T252 88L243 52Q243 48 252 48T311 46H328Q360 46 379 47T428 54T478 72T522 106T564 161Q580 191 594 228T611 270Q616 273 628 273H641Q647 264 647 262T627 203T583 83T557 9Q555 4 553 3T537 0T494 -1Q483 -1 418 -1T294 0H116Q32 0 32 10Q32 17 34 24Q39 43 44 45Q48 46 59 46H65Q92 46 125 49Q139 52 144 61Q147 65 216 339T285 628Q285 635 228 637Z"></path></g></g></g></g><rect width="1817.2" height="60" x="120" y="220"></rect></g></g></g></svg></mjx-container>表示的损失函数到达 L 的梯度，小括号中的1表明短路机制可以无损地传播梯度，而另外一项残差梯度则需要经过带有weights的层，梯度不是直接传递过来的。残差梯度不会那么巧全为-1，而且就算其比较小，有1的存在也不会导致梯度消失。所以残差学习会更容易。</p>
<script type="math/tex; mode=display">\frac{\partial loss}{\partial x_{l}}=\frac{\partial loss}{\partial x_{L}}\cdot\frac{\partial x_{L}}{\partial x_{l}}=\frac{\partial loss}{\partial x_{L}}\cdot\left(1+\frac{\partial}{\partial x_{L}}\sum_{i=l}^{L-1}F(x_{i},W_{i})\right)</script><blockquote>
<p>上述推导不严格, 只是助于理解</p>
</blockquote>
<h2 id="3-6-LSTM"><a href="#3-6-LSTM" class="headerlink" title="3.6 LSTM"></a>3.6 LSTM</h2><p>TODO: 待补充<br><img src="https://s2.loli.net/2024/03/25/ypv2Nz8hk1KBEXe.png" width="100%"></p>
<h1 id="4-神经网络权重初始化"><a href="#4-神经网络权重初始化" class="headerlink" title="4. 神经网络权重初始化"></a>4. 神经网络权重初始化</h1><ul>
<li>初始化为0</li>
<li>随机初始化</li>
<li>Xavier initialization</li>
<li>He initialization<br>其中初始化为0不可取, 那么每一层的神经元学到的东西都是一样的（输出是一样的），而且在bp的时候，每一层内的神经元也是相同的，因为他们的gradient相同。 随机初始化时, 需要乘一个值(eg.0.01)是因为要把W随机初始化到一个相对较小的值, 防止遇到如sigmoid函数会导致costfunction里的log是0. 缺点是: np.random.randn()其实是一个均值为0，方差为1的高斯分布中采样。当神经网络的层数增多时，会发现越往后面的层的激活函数（使用tanH）的输出值几乎都接近于0</li>
</ul>
<h2 id="4-1-Xavier"><a href="#4-1-Xavier" class="headerlink" title="4.1 Xavier"></a>4.1 Xavier</h2><p>思想: 尽可能的让输入和输出服从相同的分布，这样就能够避免后面层的激活函数的输出值趋向于0<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">np.random.seed(<span class="number">3</span>)</span><br><span class="line">parameters = {}</span><br><span class="line">L = <span class="built_in">len</span>(layers_dims)  <span class="comment"># integer representing the number of layers</span></span><br><span class="line"><span class="keyword">for</span> l <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">1</span>, L):</span><br><span class="line">    parameters[<span class="string">'W'</span> + <span class="built_in">str</span>(l)] = np.random.randn(layers_dims[l], layers_dims[l - <span class="number">1</span>]) * np.sqrt(<span class="number">1</span> / layers_dims[l - <span class="number">1</span>])</span><br><span class="line">    parameters[<span class="string">'b'</span> + <span class="built_in">str</span>(l)] = np.zeros((layers_dims[l], <span class="number">1</span>))</span><br><span class="line"><span class="keyword">return</span> parameters</span><br></pre></td></tr></table></figure><br>虽然Xavier initialization能够很好的 tanH 激活函数，但是对于目前神经网络中最常用的ReLU激活函数, 还是会趋向于0</p>
<h2 id="4-2-He-initialization"><a href="#4-2-He-initialization" class="headerlink" title="4.2 He initialization"></a>4.2 He initialization</h2><p>一种针对ReLU的初始化方法<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">np.random.seed(<span class="number">3</span>)</span><br><span class="line">parameters = {}</span><br><span class="line">L = <span class="built_in">len</span>(layers_dims)  <span class="comment"># integer representing the number of layers</span></span><br><span class="line"><span class="keyword">for</span> l <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">1</span>, L):</span><br><span class="line">    parameters[<span class="string">'W'</span> + <span class="built_in">str</span>(l)] = np.random.randn(layers_dims[l], layers_dims[l - <span class="number">1</span>]) * np.sqrt(<span class="number">2</span> / layers_dims[l - <span class="number">1</span>])</span><br><span class="line">    parameters[<span class="string">'b'</span> + <span class="built_in">str</span>(l)] = np.zeros((layers_dims[l], <span class="number">1</span>))</span><br><span class="line"><span class="keyword">return</span> parameters</span><br></pre></td></tr></table></figure></p>
<h2 id=""><a href="#" class="headerlink" title=" "></a> </h2><p>参考文献<br><a target="_blank" rel="noopener" href="https://blog.csdn.net/u012328159/article/details/80025785">深度学习中神经网络的几种权重初始化方法_神经网络权重初始化方法-CSDN博客</a></p>

    </div>

    
    
    

    <footer class="post-footer">

        

          <div class="post-nav">
            <div class="post-nav-item">
                <a href="/2024/03/24/%E8%83%8C%E5%8C%85%E9%97%AE%E9%A2%98/" rel="prev" title="背包问题">
                  <i class="fa fa-angle-left"></i> 背包问题
                </a>
            </div>
            <div class="post-nav-item">
                <a href="/2024/03/26/latex%E8%AF%AD%E6%B3%95/" rel="next" title="latex语法">
                  latex语法 <i class="fa fa-angle-right"></i>
                </a>
            </div>
          </div>
    </footer>
  </article>
</div>






</div>
  </main>

  <footer class="footer">
    <div class="footer-inner">

  <div class="copyright">
    &copy; 
    <span itemprop="copyrightYear">2024</span>
    <span class="with-love">
      <i class="fa fa-pencil"></i>
    </span>
    <span class="author" itemprop="copyrightHolder">Joy Zhu</span>
  </div>

    </div>
  </footer>

  
  <div class="back-to-top" role="button" aria-label="Back to top">
    <i class="fa fa-arrow-up fa-lg"></i>
    <span>0%</span>
  </div>
  <div class="reading-progress-bar"></div>

<noscript>
  <div class="noscript-warning">Theme NexT works best with JavaScript enabled</div>
</noscript>


  
  <script src="https://cdnjs.cloudflare.com/ajax/libs/animejs/3.2.1/anime.min.js" integrity="sha256-XL2inqUJaslATFnHdJOi9GfQ60on8Wx1C2H8DYiN1xY=" crossorigin="anonymous"></script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/fancyapps-ui/5.0.33/fancybox/fancybox.umd.js" integrity="sha256-+2+qOqR8CKoHh/AsVR9k2qaDBKWjYNC2nozhYmv5j9k=" crossorigin="anonymous"></script>
<script src="/js/comments.js"></script><script src="/js/utils.js"></script><script src="/js/motion.js"></script><script src="/js/next-boot.js"></script>

<script src="/js/jquery.min.js"></script><script src="/js/code-unfold.js"></script>


  <script src="https://cdnjs.cloudflare.com/ajax/libs/hexo-generator-searchdb/1.4.1/search.js" integrity="sha256-1kfA5uHPf65M5cphT2dvymhkuyHPQp5A53EGZOnOLmc=" crossorigin="anonymous"></script>
<script src="/js/third-party/search/local-search.js"></script>


  <script class="next-config" data-name="mermaid" type="application/json">{"enable":true,"theme":{"light":"default","dark":"dark"},"js":{"url":"https://cdnjs.cloudflare.com/ajax/libs/mermaid/10.7.0/mermaid.min.js","integrity":"sha256-TtLOdUA8mstPoO6sGvHIGx2ceXrrX4KgIItO06XOn8A="}}</script>
  <script src="/js/third-party/tags/mermaid.js"></script>


  <script src="/js/third-party/fancybox.js"></script>



  




  

  <script class="next-config" data-name="enableMath" type="application/json">true</script><script class="next-config" data-name="mathjax" type="application/json">{"enable":true,"tags":"none","js":{"url":"https://cdnjs.cloudflare.com/ajax/libs/mathjax/3.2.2/es5/tex-mml-chtml.js","integrity":"sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI="}}</script>
<script src="/js/third-party/math/mathjax.js"></script>



</body>
</html>
